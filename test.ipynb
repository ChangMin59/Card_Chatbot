{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë“œ ì¤‘... (facebook/opt-1.3b)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47487/3827639429.py:46: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/tmp/ipykernel_47487/3827639429.py:68: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  result = qa_chain({\"query\": query})\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìƒˆë¡œìš´ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...\n",
      "\n",
      "ğŸ”¹ AIì˜ ë‹µë³€:\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "RAGëŠ” ê¸°ì¡´ ê²€ìƒ‰ ì—”ì§„ê³¼ ë‹¤ë¥´ê²Œ, ë²¡í„° ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ ë¬¸ë§¥ì„ ë” ì˜ ì´í•´í•©ë‹ˆë‹¤.\n",
      "\n",
      "RAGëŠ” ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ AI ë‹µë³€ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ì„ í†µí•´ ë” ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "LangChainì€ ê¸°ì—…ìš© AI ê°œë°œì—ë„ í™œìš©ë©ë‹ˆë‹¤. ì±—ë´‡, ë°ì´í„° ë¶„ì„, ê²€ìƒ‰ ì—”ì§„ ë“±ì˜ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "LangChainì„ ì‚¬ìš©í•˜ë©´ ë‹¤ì–‘í•œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ AIê°€ ë” ì¢‹ì€ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "Question: RAGë€?\n",
      "Helpful Answer:\n",
      "\n",
      "ì €ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì—¬ ì •ë³´ï¿½\n",
      "\n",
      "ğŸ”¹ ê²€ìƒ‰ëœ ë¬¸ì„œ:\n",
      "- RAGëŠ” ê¸°ì¡´ ê²€ìƒ‰ ì—”ì§„ê³¼ ë‹¤ë¥´ê²Œ, ë²¡í„° ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ ë¬¸ë§¥ì„ ë” ì˜ ì´í•´í•©ë‹ˆë‹¤.\n",
      "- RAGëŠ” ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ AI ë‹µë³€ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ì„ í†µí•´ ë” ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "- LangChainì€ ê¸°ì—…ìš© AI ê°œë°œì—ë„ í™œìš©ë©ë‹ˆë‹¤. ì±—ë´‡, ë°ì´í„° ë¶„ì„, ê²€ìƒ‰ ì—”ì§„ ë“±ì˜ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "- LangChainì„ ì‚¬ìš©í•˜ë©´ ë‹¤ì–‘í•œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ AIê°€ ë” ì¢‹ì€ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.documents import Document\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "import os\n",
    "\n",
    "# 1. Hugging Face ë‹¤ìš´ë¡œë“œ ì—†ì´ ì‹¤í–‰ ê°€ëŠ¥í•œ ëª¨ë¸ ì„¤ì •\n",
    "MODEL_NAME = \"facebook/opt-1.3b\"  # ë” ê°€ë²¼ìš´ ëª¨ë¸ ì‚¬ìš© (1.3B)\n",
    "\n",
    "# 2. 4bit ì–‘ìí™” ì„¤ì • (RAM ì ˆì•½)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # 4bit ì—°ì‚°ì„ float16ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”\n",
    "    bnb_4bit_use_double_quant=True  # ë”ë¸” ì–‘ìí™” ì ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    ")\n",
    "\n",
    "# 3. Hugging Face í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ (ë‹¤ìš´ë¡œë“œ ì—†ì´ ì‹¤í–‰)\n",
    "print(\"ëª¨ë¸ ë¡œë“œ ì¤‘... (facebook/opt-1.3b)\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME\n",
    "    # ìµœì‹  ë°©ì‹ìœ¼ë¡œ 4bit ì ìš©\n",
    ")\n",
    "print(\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "# 4. LangChainìš© LLM íŒŒì´í”„ë¼ì¸ ë³€í™˜\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "# 5. ë¬¸ì„œ ë°ì´í„° ì¶”ê°€ (ë” ë§ì€ ì •ë³´ í¬í•¨)\n",
    "docs = [\n",
    "    \"LangChainì€ AI ê°œë°œì„ ì‰½ê²Œ í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. Python ê¸°ë°˜ì´ë©°, LLMì„ ì—°ê²°í•˜ì—¬ ìì—°ì–´ ì²˜ë¦¬ AIë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"RAGëŠ” ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ AI ë‹µë³€ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ì„ í†µí•´ ë” ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\",\n",
    "    \"LangChainì€ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. SQL, CSV, JSON ê°™ì€ ì™¸ë¶€ ë°ì´í„°ë„ ì§€ì›í•©ë‹ˆë‹¤.\",\n",
    "    \"RAGëŠ” GPT ëª¨ë¸ê³¼ ê²°í•©í•˜ë©´ ë”ìš± ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. OpenAI GPT-4ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ì •í™•ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤.\",\n",
    "    \"LangChainì€ ê¸°ì—…ìš© AI ê°œë°œì—ë„ í™œìš©ë©ë‹ˆë‹¤. ì±—ë´‡, ë°ì´í„° ë¶„ì„, ê²€ìƒ‰ ì—”ì§„ ë“±ì˜ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\",\n",
    "    \"RAGëŠ” ê¸°ì¡´ ê²€ìƒ‰ ì—”ì§„ê³¼ ë‹¤ë¥´ê²Œ, ë²¡í„° ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ ë¬¸ë§¥ì„ ë” ì˜ ì´í•´í•©ë‹ˆë‹¤.\",\n",
    "    \"LangChainì€ OpenAI, Hugging Face, Cohere ê°™ì€ ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"LangChainì„ ì‚¬ìš©í•˜ë©´ ë‹¤ì–‘í•œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ AIê°€ ë” ì¢‹ì€ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "]\n",
    "documents = [Document(page_content=text) for text in docs]\n",
    "\n",
    "# 6. Hugging Face Embeddings ìƒì„± (ë²¡í„° ë³€í™˜)\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# 7. FAISS ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ë° ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€\n",
    "faiss_index_path = \"faiss_index\"\n",
    "\n",
    "if os.path.exists(faiss_index_path):\n",
    "    print(\"ê¸°ì¡´ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...\")\n",
    "    vectorstore = FAISS.load_local(faiss_index_path, embeddings)\n",
    "else:\n",
    "    print(\"ìƒˆë¡œìš´ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...\")\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    vectorstore.save_local(faiss_index_path)  # ë²¡í„° ì €ì¥ (ì¬ì‚¬ìš© ê°€ëŠ¥)\n",
    "\n",
    "# 8. RetrievalQA ì²´ì¸ ì„¤ì • (ê²€ìƒ‰ + ìƒì„± ê²°í•©)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True  # ğŸ”¹ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í•¨ê»˜ ë°˜í™˜\n",
    ")\n",
    "\n",
    "# 9. ì‚¬ìš©ì ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±\n",
    "query = \"RAGë€?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# 10. ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\nğŸ”¹ AIì˜ ë‹µë³€:\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥\n",
    "print(\"\\nğŸ”¹ ê²€ìƒ‰ëœ ë¬¸ì„œ:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"- {doc.page_content}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
