import os
import json
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain.memory import ConversationBufferMemory
from langchain_core.output_parsers import StrOutputParser
from sentence_transformers import CrossEncoder # ë¦¬ëž­ì»¤
from langchain_community.tools import TavilySearchResults
from langchain.llms import HuggingFacePipeline
#from langchain_community.llms import HuggingFacePipeline
# for huggingface
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

os.environ["TAVILY_API_KEY"] = os.getenv("TAVILY_API_KEY", "")


# === ì¹´ë“œ ë°ì´í„° ë¡œë“œ ===
DATA_PATH = os.getenv("CARD_DATA", "data/card_llm_ready.json")
with open(DATA_PATH, "r", encoding="utf-8") as f:
    raw_data = json.load(f)

# === ì¹´ë“œ ë¬¸ì„œ ë³€í™˜ ===
docs = [
    Document(
        page_content=f"[{item['ì¹´ë“œ ì´ë¦„']}] - {item['ì¹´ë“œ íšŒì‚¬']} ({item['í˜œíƒ í‚¤ì›Œë“œ']}) í˜œíƒ: {item['í˜œíƒ ì„¤ëª…']}",
        metadata={
            "ì¹´ë“œ ì´ë¦„": item["ì¹´ë“œ ì´ë¦„"],
            "ì¹´ë“œ íšŒì‚¬": item["ì¹´ë“œ íšŒì‚¬"],
            "í˜œíƒ í‚¤ì›Œë“œ": item["í˜œíƒ í‚¤ì›Œë“œ"],
            "í˜œíƒ ì„¤ëª…": item["í˜œíƒ ì„¤ëª…"]
        }
    ) for item in raw_data
]

# === ìž„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ì„¤ì • ===
embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")

#if os.path.exists("faiss_index"):
#    vectorstore = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization=True)
#else:
vectorstore = FAISS.from_documents(docs, embedding_model)
vectorstore.save_local("faiss_index")


# ë¦¬ëž­ì»¤: ê²€ìƒ‰ê¸°(retriever) ì„¤ì •. ë¬¸ì„œìˆ˜ 3123ê°œ (/home/alpaco/1t/changmin/card_llm_ready.json)
num_docs = 15     # 2 ~ 10 ê°œ, ê¸°ì¡´ main.py ì½”ë“œì—ì„œ kê°€ 15ì˜€ìœ¼ë¯€ë¡œ 15ë¡œ ë³€ê²½ 10 -> 15.
k = num_docs * 5  # 10 ~ 50 ê°œ, ì†ë„ê°€ ëŠë¦´ ê²½ìš° ì¤„ì¼ ìˆ˜ ìžˆìŒ.
fetch_k = k * 10   # kë¹„ì¤‘ 2~10 ë°°
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": k, "fetch_k": fetch_k})
retriever0 = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 15})

# ë¦¬ëž­ì»¤: ëª¨ë¸ ì´ˆê¸°í™”
reranker_model = CrossEncoder('Dongjin-kr/ko-reranker')

# ë¦¬ëž­ì»¤: ë¦¬íŠ¸ë¦¬ë²„ì— ë¦¬ëž­ì»¤ë¥¼ ê²°í•©í•œ ê²€ìƒ‰ í•¨ìˆ˜
def advanced_retrieval(query):
    # ì´ˆê¸° ìœ ì‚¬ë„ ê²€ìƒ‰
    initial_results = retriever.invoke(query)

    if not initial_results:
        return []
    
    # ë¦¬ëž­ì»¤ë¥¼ ì‚¬ìš©í•œ ìž¬ì •ë ¬: query(ì§ˆë¬¸), page_content(ë¦¬íŠ¸ë¦¬ë²„ê°€ ê°€ì ¸ì˜¨ doc)
    pairs = [[query, doc.page_content] for doc in initial_results]
    reranked_scores = reranker_model.predict(pairs)
    
    #ì ìˆ˜ì— ë”°ë¼ ë¬¸ì„œ ìž¬ì •ë ¬
    reranked_docs = [
        doc for _, doc in sorted(
            zip(reranked_scores, initial_results), 
            key=lambda x: x[0], 
            reverse=True
        )
    ]

    return reranked_docs[:num_docs]  # ìƒìœ„ ë¬¸ì„œë§Œ ë°˜í™˜

# ë¦¬ëž­ì»¤: ê²€ìƒ‰í•¨ìˆ˜ í• ë‹¹
reranker = RunnableLambda(advanced_retrieval)


# === ë¬¸ë§¥ í¬ë§· í•¨ìˆ˜ ===
def format_docs(docs):
    if not docs:
        return "ê´€ë ¨ëœ ì¹´ë“œë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”."
    seen = set()
    formatted = []
    for i, doc in enumerate(docs, 1):
        name = doc.metadata["ì¹´ë“œ ì´ë¦„"]
        company = doc.metadata["ì¹´ë“œ íšŒì‚¬"]
        benefit = doc.page_content.split("í˜œíƒ: ")[-1] if "í˜œíƒ: " in doc.page_content else "í˜œíƒ ì •ë³´ ì—†ìŒ"
        if name not in seen:
            seen.add(name)
            formatted.append(f"{i}. {name} ({company}) - í˜œíƒ: {benefit}")
    return "\n".join(formatted)

# === ëŒ€í™” ë©”ëª¨ë¦¬ ===
memory = ConversationBufferMemory(
    memory_key="chat_history",
    input_key="question",
    output_key="output",
    return_messages=True
)

def get_chat_history(_):
    return "\n".join(
        [f"{msg.type.capitalize()}: {msg.content}" for msg in memory.load_memory_variables({})["chat_history"]]
    )

# === LLM ë¡œë”© ===
llm_model = os.getenv("LLM_MODEL", "mistral:latest")
llm = Ollama(model=llm_model)
merged_model_path = "./aris"
model = AutoModelForCausalLM.from_pretrained(merged_model_path, device_map="auto",torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(merged_model_path)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=100,
    #temperature=0.7,
    #top_p=0.9,
    #repetition_penalty=1.1,
    truncation=True,
    do_sample=False,
    use_cache=True
)
llm_aris = HuggingFacePipeline(pipeline=pipe)

# ì§ˆë¬¸ ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸
classification_prompt = ChatPromptTemplate.from_template(
    """
ë‹¤ìŒ ì‚¬ìš©ìž ì§ˆë¬¸ì´ ì–´ë–¤ ëª©ì ì— í•´ë‹¹í•˜ëŠ”ì§€ íŒë‹¨í•´ì¤˜.
ì´ 4ê°€ì§€ ìœ í˜•ì´ ìžˆì–´. ì„¤ëª…ê³¼ ì˜ˆì‹œë¥¼ ìž˜ ì°¸ê³ í•´ì„œ ê°€ìž¥ ì ì ˆí•œ ë²ˆí˜¸ í•˜ë‚˜ë§Œ ê³¨ë¼ì¤˜.

â€» ë‹µë³€ì€ ë°˜ë“œì‹œ ìˆ«ìž í•˜ë‚˜ë§Œ ì¤˜ (ì˜ˆ: 1). ì„¤ëª…ì´ë‚˜ ì´ìœ ëŠ” ì ˆëŒ€ ì“°ì§€ ë§ˆ.
â€» ê°€ëŠ¥í•œ ê²½ìš° 1~3ë²ˆ ì¤‘ì—ì„œ ì„ íƒí•˜ë ¤ê³  ë…¸ë ¥í•´ì¤˜.
â€» 4ë²ˆì€ ì¹´ë“œëž‘ ì •ë§ ê´€ë ¨ ì—†ëŠ” ìž¡ë‹´ì¼ ë•Œë§Œ ê³¨ë¼ì¤˜.

[ì¹´í…Œê³ ë¦¬ ìœ í˜•]

1. íŠ¹ì • ìƒí™©ì´ë‚˜ ëŒ€ìƒì— ë§žëŠ” ì¹´ë“œ ì¶”ì²œ  
â†’ ì‚¬ìš©ìžê°€ ë³¸ì¸ì˜ ìƒí™©(ì˜ˆ: ì£¼ìœ ì†Œ ìžì£¼ ì´ìš©, ì˜¨ë¼ì¸ ì‡¼í•‘ ìžì£¼ í•¨ ë“±)ì„ ë§í•˜ë©°  
  ì¹´ë“œ ì¢…ë¥˜ë¥¼ ì–´ëŠ ì •ë„ ì•Œê³  ìžˆëŠ” ê²½ìš°  
â†’ ì‚¬ìš©ìžê°€ "ì´ëŸ° ìƒí™©ì— ë§žëŠ” ì¹´ë“œ ì•Œë ¤ì¤˜"ë¼ëŠ” ì‹ìœ¼ë¡œ ì§ˆë¬¸í•¨

ì˜ˆì‹œ:
- ì£¼ìœ ì†Œ í• ì¸ë˜ëŠ” ì¹´ë“œ ì•Œë ¤ì¤˜  
- 20ëŒ€ ì—¬ì„± ì§ìž¥ì¸ì—ê²Œ ì¢‹ì€ ì¹´ë“œ ì¶”ì²œí•´ì¤˜  
- ì¹´íŽ˜ ìžì£¼ ê°€ëŠ” ì‚¬ëžŒì—ê²Œ í˜œíƒ ì¢‹ì€ ì¹´ë“œ ë­ ìžˆì–´?  
- ë³‘ì› ìžì£¼ ë‹¤ë‹ˆëŠ” ì‚¬ëžŒì—ê²Œ ìœ ë¦¬í•œ ì¹´ë“œ ìžˆì–´?  
- ì—¬í–‰ ìžì£¼ ê°€ëŠ” ì‚¬ëžŒì—ê²Œ ì¢‹ì€ ì¹´ë“œ ì¶”ì²œí•´ì¤˜  

2. íŠ¹ì • ì¹´ë“œ í˜œíƒ ì¡°íšŒ ë˜ëŠ” ì¹´ë“œ ê°„ ë¹„êµ  
â†’ ì¹´ë“œ ì´ë¦„ì´ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë“±ìž¥í•˜ë©°  
  í•´ë‹¹ ì¹´ë“œì— ëŒ€í•œ ì •ë³´ë‚˜ ë¹„êµë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš°

ì˜ˆì‹œ:
- ì‚¼ì„±ì¹´ë“œ the O í˜œíƒ ë­ì•¼?  
- í˜„ëŒ€ the Greenì´ëž‘ ì‹ í•œ Deep Oil ë¹„êµí•´ì¤˜  
- Deep Oilì´ëž‘ Deep On ì¹´ë“œ ì°¨ì´ì ì€ ë­ì•¼?  
- ì´ë””ì•¼ í• ì¸ë˜ëŠ” ì¹´ë“œê°€ ë­ì•¼?  

3. ì‚¬ìš©ìž ë§žì¶¤ ì¹´ë“œ ì¶”ì²œ (ì—­ì§ˆë¬¸ í•„ìš”)  
â†’ ì‚¬ìš©ìžê°€ ë³¸ì¸ì˜ ìƒí™©ì„ êµ¬ì²´ì ìœ¼ë¡œ ë§í•˜ì§€ ì•Šê±°ë‚˜  
  ì–´ë–¤ ì¹´ë“œë¥¼ ì„ íƒí•´ì•¼ í• ì§€ ëª¨ë¥´ëŠ” ìƒíƒœ  
â†’ ì´ ê²½ìš°ì—ëŠ” ì„±ë³„, ë‚˜ì´, ê´€ì‹¬ì‚¬ ë“±ì„ ë˜ë¬¼ì–´ì•¼ í•˜ë¯€ë¡œ  
  **ì—­ì§ˆë¬¸ì´ í•„ìš”í•œ ê²½ìš°ë¡œ íŒë‹¨**

ì˜ˆì‹œ:
- ë‚˜í•œí…Œ ë§žëŠ” ì¹´ë“œ ì¶”ì²œí•´ì¤˜  
- ì–´ë–¤ ì¹´ë“œê°€ ë‚˜ëž‘ ìž˜ ë§žì„ê¹Œ?  
- ì¹´ë“œ í•˜ë‚˜ ë§Œë“¤ê¹Œ í•˜ëŠ”ë° ë­ê°€ ì¢‹ì„ê¹Œ?  
- ìš”ì¦˜ ì¹´ë“œ ë­ê°€ ìž˜ ë‚˜ê°€? ë‚˜í•œí…Œ ì–´ìš¸ë¦¬ëŠ” ê±° ì¶”ì²œí•´ì¤˜  
- ì‹ ìš©ì¹´ë“œ ì²˜ìŒ ë§Œë“¤ê±´ë° ì–´ë””ì„œë¶€í„° ì‹œìž‘í•´ì•¼ í• ê¹Œ?  
- ì•„ë¬´ ì¹´ë“œë‚˜ ì“°ê¸° ì‹«ì€ë° ë‚˜í•œí…Œ ë§žëŠ” ì¹´ë“œê°€ í•„ìš”í•´  
- ì‹ ìš©ì¹´ë“œ ìž˜ ëª°ë¼ì„œ ì¶”ì²œí•´ì¤˜  
- ìƒí™©ì— ë§žëŠ” ê±¸ ì¶”ì²œí•´ì¤¬ìœ¼ë©´ ì¢‹ê² ì–´  

4. ì¹´ë“œì™€ ë¬´ê´€í•œ ì§ˆë¬¸ (ìž¡ë‹´ ë“±)  
â†’ ì‹ ìš©ì¹´ë“œì™€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ ë˜ëŠ” ì¼ë°˜ì ì¸ ëŒ€í™”

ì˜ˆì‹œ:
- ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?  
- ë„Œ ëˆ„êµ¬ì•¼?  
- ì‹¬ì‹¬í•œë° ë­í•˜ì§€?  
- ë°¥ ë­ ë¨¹ì„ê¹Œ?  
- ë„ˆ ëª‡ ì‚´ì´ì•¼?  

ì§ˆë¬¸: {question}  
ë‹µë³€ (ìˆ«ìž í•˜ë‚˜ë§Œ):
"""
)

classifier_chain = classification_prompt | llm | StrOutputParser()

# ê°ê°ì˜ ì‘ë‹µ ë°©ì‹ ì •ì˜
# 1ë²ˆ ì‘ë‹µ: ê¸°ë³¸ ì¹´ë“œ ì¶”ì²œ ë°©ì‹
card_recommend_template = ChatPromptTemplate.from_template("""
ë„ˆëŠ” í•œêµ­ì–´ë¡œ ì‹ ìš©ì¹´ë“œ í˜œíƒì„ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ëŠ” ì „ë¬¸ê°€ì•¼.

- ì²˜ìŒì—ëŠ” ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ê´€ë ¨ëœ ì‹ ìš©ì¹´ë“œ ëª©ë¡ì„ ë³´ì—¬ì¤˜. ê° ì¹´ë“œë³„ë¡œ ì´ë¦„ê³¼ ì£¼ìš” í˜œíƒì„ ê°„ëžµí•˜ê²Œ ìš”ì•½í•´ì¤˜.
- ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ì¹´ë“œ ê°œìˆ˜ë¥¼ ë§í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ìµœëŒ€í•œ ë‹¤ì–‘í•œ ì˜µì…˜ì„ ë³´ì—¬ì¤˜.
- ì¹´ë“œëª…ì€ contextì— ë“±ìž¥í•œ ì‹¤ì œ ì¹´ë“œëª…ë§Œ ì‚¬ìš©í•˜ê³ , ê°„ë‹¨í•˜ê³  ëª…í™•í•œ í‘œí˜„ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.
- ë¦¬ìŠ¤íŠ¸ê°€ ëë‚˜ë©´, ê·¸ ì•ˆì—ì„œ **ê°€ìž¥ í˜œíƒì´ ì¢‹ì€ ì¹´ë“œ 1~2ê°œë§Œ ì¶”ì²œ**í•´
- ë§ˆì§€ë§‰ì—ëŠ” í˜œíƒ ë¹„êµ ë° ì¶”ì²œí•´ì¤˜ ì¶”ì²œí•œ ê¸°ì¤€ë„ ê°„ë‹¨ížˆ ì •ë¦¬í•´ì¤˜

# ì¶”ì²œ ì‹ ìš©ì¹´ë“œ ëª©ë¡:
{context}

# ì´ì „ ëŒ€í™”:
{chat_history}

# ì§ˆë¬¸:
{question}

# ë‹µë³€:
""")
recommend_chain = {
        "context": reranker | RunnableLambda(format_docs), # ë¦¬ëž­ì»¤: retriever0 -> reranker
        "chat_history": RunnableLambda(get_chat_history),
        "question": RunnablePassthrough(),
} | card_recommend_template | llm

# 2ë²ˆ ì‘ë‹µ: ì¹´ë“œ í˜œíƒ ì¡°íšŒ ë° ë¹„êµ
card_info_template = ChatPromptTemplate.from_template("""
ë„ˆëŠ” í•œêµ­ì–´ë¡œ ì‹ ìš©ì¹´ë“œ í˜œíƒì„ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ëŠ” ì „ë¬¸ê°€ì•¼.

- ì²˜ìŒì—ëŠ” ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ê´€ë ¨ëœ ì‹ ìš©ì¹´ë“œ ëª©ë¡ì„ ë³´ì—¬ì¤˜. ê° ì¹´ë“œë³„ë¡œ ì´ë¦„ê³¼ ì£¼ìš” í˜œíƒì„ ê°„ëžµí•˜ê²Œ ìš”ì•½í•´ì¤˜.
- ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ì¹´ë“œ ê°œìˆ˜ë¥¼ ë§í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ìµœëŒ€í•œ ë‹¤ì–‘í•œ ì˜µì…˜ì„ ë³´ì—¬ì¤˜.
- ì¹´ë“œëª…ì€ contextì— ë“±ìž¥í•œ ì‹¤ì œ ì¹´ë“œëª…ë§Œ ì‚¬ìš©í•˜ê³ , ê°„ë‹¨í•˜ê³  ëª…í™•í•œ í‘œí˜„ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.
- ë¦¬ìŠ¤íŠ¸ê°€ ëë‚˜ë©´, ê·¸ ì•ˆì—ì„œ **ê°€ìž¥ í˜œíƒì´ ì¢‹ì€ ì¹´ë“œ 1~2ê°œë§Œ ì¶”ì²œ**í•´
- ë§ˆì§€ë§‰ì—ëŠ” í˜œíƒ ë¹„êµ ë° ì¶”ì²œí•´ì¤˜ ì¶”ì²œí•œ ê¸°ì¤€ë„ ê°„ë‹¨ížˆ ì •ë¦¬í•´ì¤˜

# ì¶”ì²œ ì‹ ìš©ì¹´ë“œ ëª©ë¡:
{context}

# ì´ì „ ëŒ€í™”:
{chat_history}

# ì§ˆë¬¸:
{question}

# ë‹µë³€:
""")
compare_chain = {
        "context": reranker | RunnableLambda(format_docs), # ë¦¬ëž­ì»¤: retriever0 -> reranker
        "chat_history": RunnableLambda(get_chat_history),
        "question": RunnablePassthrough(),
} | card_info_template | llm

# 3ë²ˆ ì‘ë‹µ: ì‚¬ìš©ìž ë§žì¶¤ ì¶”ì²œ (ì—­ì§ˆë¬¸)
def ask_user_profile():
    answers = {}
    questions = [
        "ê·¸ë ‡ë‹¤ë©´ ì¹´ë“œë¥¼ ì¶”ì²œë“œë¦¬ê¸°ì— ì•žì„œ ëª‡ê°€ì§€ ì§ˆë¬¸ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë‚˜ì´ê°€ ì–´ë–»ê²Œ ë˜ì‹­ë‹ˆê¹Œ?",
        "ì„±ë³„ì€ ë¬´ì—‡ì´ì‹ ê°€ìš”?",
        "ì§ì—…ì€ ë¬´ì—‡ì´ì‹ ê°€ìš”?",
        "ë§ˆì§€ë§‰ìœ¼ë¡œ ê´€ì‹¬ì‚¬ë‚˜ ì·¨ë¯¸ê°€ ìžˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”. ì˜í™”/ì±…/ë“œë¼ì´ë¸Œ/ì• ì™„ë™ë¬¼ ë“± ë¬´ì—‡ì´ë“  ì¢‹ìŠµë‹ˆë‹¤."
    ]
    for q in questions:
        answers[q] = input(f"{q} ")
    profile = "\n".join([f"{k} {v}" for k, v in answers.items()])
    prompt = f"""

ë„ˆëŠ” í•œêµ­ì–´ë¡œ ì‹ ìš©ì¹´ë“œ í˜œíƒì„ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ëŠ” ì „ë¬¸ê°€ì•¼.

-ì•„ëž˜ ì‚¬ìš©ìž í”„ë¡œí•„ì„ ì°¸ê³ í•´ì„œ ê°€ìž¥ ì ì ˆí•œ ì‹ ìš©ì¹´ë“œë¥¼ 3ê°œë¥¼ ì•„ëž˜ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.
ë‹µë³€ì€ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¼ì¤˜:

1. ì¹´ë“œ ì´ë¦„
- ì£¼ìš” í˜œíƒ ìš”ì•½
- ê·¸ ì™¸ í˜œíƒ ìš”ì•½
2. ì¹´ë“œ ì´ë¦„
- ì£¼ìš” í˜œíƒ ìš”ì•½
- ê·¸ ì™¸ í˜œíƒ ìš”ì•½
...

ì‚¬ìš©ìž í”„ë¡œí•„:
{profile}

#ë‹µë³€:
"""
    return llm.invoke(prompt)

# 4ë²ˆ ì‘ë‹µ: ë¬´ê´€í•œ ì§ˆë¬¸
irrelevant_response = "ê·¸ ì§ˆë¬¸ì€ ì¹´ë“œë‚˜ í˜œíƒì´ëž‘ì€ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤."

# template for web search result.
template_for_web = """
ë„ˆëŠ” ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìž˜ ì„¤ëª…í•´ì£¼ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ê³ , ì˜ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆ.
ì•„ëž˜ ê²€ìƒ‰ ê²°ê³¼(context)ì—ì„œ ì‚¬ìš©ìž ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì•„ì„œ ì¹œì ˆí•˜ê³  êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.
ê²€ìƒ‰ ê²°ê³¼(context)ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìœ¼ë©´, ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆë‹¤ê³  ë§í•´ì¤˜.

ë‹µë³€ì€ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¼ì¤˜:
ì›¹ì—ì„œ ê²€ìƒ‰í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
...

#ì§ˆë¬¸:
{question}

#ê²€ìƒ‰ ê²°ê³¼:
{context}

#ë‹µë³€:
"""

prompt_for_web = ChatPromptTemplate.from_template(template_for_web)

web_chain = {
        "context": RunnablePassthrough(), # ë¦¬ëž­ì»¤: retriever0 -> reranker
        #"chat_history": RunnableLambda(get_chat_history),
        "question": RunnablePassthrough(),
} | prompt_for_web | llm

def enhance_query(query):
    if len(query.strip()) < 2:
        return query + " ì˜ë¯¸"  # ë‹¨ì–´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ "ì˜ë¯¸"ë¥¼ ì¶”ê°€
    return query



# === ì‹¤í–‰ë¶€ ===
if __name__ == "__main__":
    print("\nðŸ’³ ì‹ ìš©ì¹´ë“œ GPT ì°¨íŠ¸ë° (ìµœì¢… ë²„ì „)")
    while True:
        user_input = input("\nì§ˆë¬¸ ìž…ë ¥ ('q' ë˜ëŠ” 'ì¢…ë£Œ' ìž…ë ¥ ì‹œ ì¢…ë£Œ): ").strip()

        if user_input.lower() in ["q", "ì¢…ë£Œ"]:
            print("\nì°¨íŠ¸ë°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
            break

        try:
            category = classifier_chain.invoke({"question": user_input}).strip()

            if category == "1":
                result = recommend_chain.invoke(user_input)

            elif category == "2":
                result = compare_chain.invoke(user_input)

            elif category == "3":
                result = ask_user_profile()

            else:
                print("4ë²ˆ")
                search_query = user_input
                search_query = enhance_query(search_query)
                web_search = TavilySearchResults(max_results=2)
                search_results = web_search.invoke(search_query)
                search_result = ''
                for result in search_results:
                    search_result += result['content'] + '\n'
                result = web_chain.invoke(
                    {
                        "question": user_input,
                        "context": search_result,
                    }
                )
            print("\në‹µë³€:", result, "\n")
            memory.save_context({"question": user_input}, {"output": result})

        except Exception as e:
            print("ì˜¤ë¥˜ ë°œìƒ:", str(e))
            